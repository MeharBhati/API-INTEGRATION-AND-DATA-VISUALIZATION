# API-INTEGRATION-AND-DATA-VISUALIZATION

Company : CODETECH IT SOLUTIONS

Name : Bhati Mehar Mansoor Akhtar

Intern ID : CT04DH2154

Domain : Python 

Duration : 4 weeks

Mentor : Neela Santosh

****************************************

During my Python internship with CODTECH, I successfully completed four key tasks that demonstrated my skills in API integration, data analysis, automation, chatbot development, and machine learning.

Task 1 – API Integration and Data Visualization
I developed a Python script to fetch real-time data from a public API (such as OpenWeatherMap). The script retrieved structured JSON data, processed it, and visualized the results using Matplotlib and Seaborn. This task involved working with the requests library for API calls, data parsing using Pandas, and creating informative graphs for better insights. The final deliverable was a visualization dashboard showcasing the fetched data trends.

Task 2 – Automated Report Generation
For this task, I created a script that read and analyzed data from a file (e.g., CSV or text format). After performing statistical calculations and extracting meaningful patterns, I generated a well-formatted PDF report using FPDF and ReportLab libraries. The process included designing headings, tables, and charts in the PDF for better readability. This automated workflow eliminated manual formatting and streamlined reporting.

Task 3 – AI Chatbot with NLP
I built a chatbot using Natural Language Processing (NLP) libraries such as NLTK and spaCy. The chatbot could understand user queries, process them, and respond appropriately. Key steps included tokenizing text, applying lemmatization, and implementing intent recognition. This project enhanced my understanding of text preprocessing, conversational logic, and building interactive Python applications.

Task 4 – Machine Learning Model Implementation
Using Scikit-learn, I developed a predictive model to classify or predict outcomes from a dataset (e.g., spam email detection). I performed data preprocessing, feature selection, model training, and evaluation using techniques like train-test split, accuracy scoring, and confusion matrices. The implementation was carried out in Jupyter Notebook, making the process transparent and reproducible with both code and output documentation.

Tools & Technologies I have Used:

Programming Language: Python

Libraries for Data Handling & Visualization: Pandas, Matplotlib, Seaborn

API Handling: Requests

PDF Generation: FPDF, ReportLab

Natural Language Processing: NLTK, spaCy

Machine Learning: Scikit-learn

Development Environment: Jupyter Notebook, Visual studio code.

Version Control: GitHub for storing code and reports


***Output**

<img width="1678" height="1182" alt="Image" src="https://github.com/user-attachments/assets/419459b7-0c3d-4b35-b859-36512a798aa2" />

<img width="970" height="752" alt="Image" src="https://github.com/user-attachments/assets/e28270c8-2cc6-47f4-aa59-63a08364bf78" />

<img width="1262" height="525" alt="Image" src="https://github.com/user-attachments/assets/72dc0173-8237-46f7-8c73-dc5e79c7303b" />

<img width="1192" height="865" alt="Image" src="https://github.com/user-attachments/assets/ffe2246e-0387-4375-8199-7d615a400bf0" />






















